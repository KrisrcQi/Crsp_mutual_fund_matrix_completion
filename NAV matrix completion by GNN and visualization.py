# -*- coding: utf-8 -*-
"""NAV matrix completion by GNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1S8KZ5geZtRQsjnR3bxdWFAQfXggKuGec
"""
!pip install tensorboard
!pip install jupyter_tensorboard
!pip install torch-geometric
"""
If the package of tensorboard, and torch-geometric is installed in the environment, skipping this pip process
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch_geometric.data import Data, DataLoader
from torch_geometric.nn import GCNConv

# Reading the data file
# Checking the empty cells in the dataset
df = pd.read_csv('nav_matrix.csv')
print(df)
small_date = df['date'].min()
print(small_date)
max_date = df['date'].max()
print(max_date)

df.iloc[:, 1]

# Convert the 'Date' column to datetime type
df['date'] = pd.to_datetime(df['date'])

# Convert datetime64 to float (Unix timestamp)
df['date_float'] = df['date'].astype(int) / 10**9  # Divide by 10^9 to convert nanoseconds to seconds

df
# print(type(df['date'][0]))

# Get the column names and reorder them
cols = df.columns.tolist()
cols = [cols[-1]] + cols[:-1]

# Reorder the dataframe columns
df = df[cols]

print(df)

df.drop(df.columns[1], axis=1, inplace=True)
print(df)

df.drop(df.columns[1], axis=1, inplace=True)
dprint(df)
print(type(df['date_float'][0]))
df.to_csv('nav_matrix_float_version.csv')


"""The next step is to set GNN model and training model to get the prediction 
    Additionally, by using tensorboard to visualization the training process 

"""
#GNN model 
data = df
num_features = 1

# Step 1: Convert matrix to graph representation
adjacency_matrix = torch.tensor(data.values)  # Convert matrix to tensor
edge_index = adjacency_matrix.nonzero().t()  # Get indices of non-zero values as edges
x = torch.randn(adjacency_matrix.size(0), num_features)  # Node features (randomly initialized)

#Assuming adjacency matrix is your adjacency matrix
num_nodes = adjacency_matrix.size(0) 
# Get the number of nodes
# Create a mask that is True for valid indices and False for out-of-bounds indices
mask = (edge_index[0] < num_nodes) & (edge_index[1] < num_nodes)
# AppIy the mask to edge index
edge_index = edge_index[:, mask]

print(adjacency_matrix)
print(edge_index)
print(x)

# Step 2: Split the dataset
# Randomly remove some values to create observed values dataset
observed_matrix = adjacency_matrix.clone()
indices = torch.randperm(adjacency_matrix.numel())[:int(0.8 * adjacency_matrix.numel())]
indices = indices[indices < 141]
observed_matrix.reshape(-1)[indices] = float('nan')
# Keep missing values as the evaluation dataset
missing_values = adjacency_matrix.clone()
missing_values.reshape(-1)[indices] = float('nan')
print(len(indices))

# # Below code is the another way to match the indices size with x 
# valid_indices = indices[indices < adjacency_matrix.numel()]
# valid_values = adjacency_matrix.flatten()[valid_indices]

# # Create a new tensor to hold the modified adjacency matrix
# updated_adjacency_matrix = adjacency_matrix.clone()

# # Convert the tensor to a 1-dimensional tensor for indexing
# flattened_indices = valid_indices.flatten()

# # Scatter the valid values into the updated_adjacency_matrix using the flattened_indices
# updated_adjacency_matrix.reshape(-1)[flattened_indices] = float('nan')

# observed_matrix = updated_adjacency_matrix

# missing_values = adjacency_matrix.clone()
# missing_values.reshape(-1)[flattened_indices] = float('nan')


# Step 3: Construct the GNN model 
class GNNModel(nn.Module):
    def __init__(self, input_dim, hidden_dim):
        super(GNNModel, self).__init__()
        self.conv1 = GCNConv(input_dim, hidden_dim)
        self.conv2 = GCNConv(hidden_dim, hidden_dim)
        self.fc = nn.Linear(hidden_dim, 1)

    def forward(self, x, edge_index):
        x = F.relu(self.conv1(x, edge_index))
        x = F.relu(self.conv2(x, edge_index))
        x = self.fc(x).squeeze()
        return x

input_dim = num_features  # Dimensionality of input features
hidden_dim = 64  # Dimensionality of hidden layers

model = GNNModel(input_dim, hidden_dim)

# Step 4: Train the GNN model and visualization the training processing by tensorboard
criterion = nn.MSELoss()  # Mean Squared Error loss
optimizer = optim.Adam(model.parameters(), lr=0.01)

data = Data(x=x, edge_index=edge_index)
loader = DataLoader([data], batch_size=64)  # DataLoader for a single graph

num_epochs = 40

# Load the TensorBoard notebook extension
%load_ext tensorboard
writer = SummaryWriter(log_dir='logs')

# Training loop
for epoch in range(num_epochs):
    model.train()
    for batch in loader:
        optimizer.zero_grad()
        output = model(batch.x, batch.edge_index)
        target = batch.x.t()  # Transpose the target tensor to match output shape
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
    print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {loss}")
    writer.add_scalar('loss', loss, global_step=epoch)
writer.close()

# viusalization the training process 
tensorboard --logdir=logs --port=6007

# Step 5: Evaluate the GNN model
model.eval()
with torch.no_grad():
    missing_values_prediction = model(x, edge_index)  # Predict missing values


# Convert the predicted missing values back to a matrix
predicted_matrix = adjacency_matrix.clone().double()  # Convert predicted_matrix to Double data type
selected_indices = indices[:missing_values_prediction.size(0)]
predicted_matrix.reshape(-1)[selected_indices] = missing_values_prediction.double()  # Convert missing_values_prediction to Double data type

# Print the predicted matrix
torch.set_printoptions(precision=2)
print(predicted_matrix)

# Convert the tensor to a NumPy array
predicted_array = predicted_matrix.numpy()

# Create a new DataFrame from the array
predicted_df = pd.DataFrame(predicted_array)

# Print the new DataFrame
print(predicted_df)
predicted_df.to_csv('predicted matrix.csv')
